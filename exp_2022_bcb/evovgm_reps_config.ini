## ##################################################################
## write_run_rep_exps.py template config file 
## 
## Hyper-parameters and default values used in the 2022 BCB paper
## ##################################################################

## ##################################################################
##  write_run_rep_exps.py configuration sections
## ##################################################################

## [slurm] and [evaluation] sections will be removed in the final
## config files for evovgm.py

[slurm]
run_slurm = False
## if run_slurm=False: the script will launch the jobs locally
## elif run_slurm=True: it will launch the jobs on slurm

## Account: UPDATE your slurm account information here
account = ctb-banire
#account = def-banire
mail_user = amine.m.remita@gmail.com

## SLURM parameters
## ################
# If gpus_per_node > 0, device in [settings] will be updated to cuda
gpus_per_node = 0
cpus_per_task = 12
exec_time = 12:00:00
mem = 75000M
## For testing
#exec_time = 00:05:00
#mem = 8000M

[evaluation]
run_jobs = True
## if run_jobs = False: the script generates the evovgm config files 
## but it won't launch the jobs. If run_jobs=True: it runs the jobs

n_epochs = 5000
nb_replicates = 10
## For testing
#n_epochs = 100
#nb_replicates = 2

## Remark: values can span multiple lines, as long as they are
## indented deeper than the first line of the value.
## Configparser fetchs the value as string, after that it will be 
## casted to a list using json.loads

## Substitution model types for evoVGM model
## #########################################
model_types = [
        "jc69",
        "k80", 
        "gtr"
        ]

## Simulation data parameters 
## ##########################
data_types = [
    ["jc69", "0.160, 0.160, 0.160, 0.160, 0.160, 0.160",
        "0.25, 0.25, 0.25, 0.25"],
    ["k80" , "0.250, 0.125, 0.125, 0.125, 0.125, 0.250",
        "0.25, 0.25, 0.25, 0.25"], 
    ["gtr" , "0.160, 0.050, 0.160, 0.090, 0.300, 0.240",
        "0.10, 0.45, 0.30, 0.15"]
    ]

# Numbers of sequences and their branch lengths
## ############################################
branches = [
    [3, "0.1,0.3,0.45"],
    [4, "0.1,0.3,0.45,0.15"],
    [5, "0.1,0.3,0.45,0.15,0.2"]
    ]

# Alignment lengths
## ################
len_alns = [
        100, 
        1000,
        5000
        ]

# ##################################################################
# EvoVGM template configuration sections
# ##################################################################
[io]
# output_path is used to save training data, scores and figures
# It will be updated to ../exp_outputs/<job_code>
# <job_code> is defined in write_run_rep_exps.py
output_path = nb_sequences

# If False the program runs the evaluation and save resutls in
# output_path,else the program loads the results directly from the
# file (if it exists).
scores_from_file = True

[data]
# Use validation data along fitting and for generating step
validation = True

# If sim_data = True: evolve new sequences
# else: fetch sequences from fasta_fit_file [and fasta_val_file]
sim_data = True

# If sim_from_fasta and sim_data are True, the data will be
# extracted from simulated FASTA files if they already exist,
# else: new alignment will be simulated
sim_from_fasta = True

## alignment_size will be updated from [evaluation] section
alignment_size = 5000

## Evo parameters
## ##############
## The parameters will be updated from [evaluation] section
##
# Branch lengths is list of M floats separated by comma (w/out space)
# M is the number of sequences to be evolved
branch_lengths = 0.1,0.3,0.45
# Substitution rates
#        AG    AC    AT    GC    GT    CT
rates = 0.16, 0.16, 0.16, 0.16, 0.16, 0.16
# Relative frequencies
#        A     C      G    T
freqs = 0.25, 0.25, 0.25, 0.25

[vb_model]
## evomodel will be updated from [evaluation] section
# jc69 := EvoVGM_JC69: infer a and b latent variables
# k80  := EvoVGM_K80 : infer a, b, k latent variables
# gtr  := EvoVGM_GTR : infer a, b, r, f latent variables
evomodel = jc69

[hyperparams]
## nb_replicates will be updated from [evaluation] section
nb_replicates = 10
alpha_kl = 0.0001
nb_samples = 100
hidden_size = 32
nb_layers = 3
sample_temp = 0.1
## n_epochs will be updated from [evaluation] section
n_epochs = 5000
# optimizer type : adam | sgd
optim=adam
learning_rate = 0.005
optim_weight_decay = 0.00001

[priors]
# Accepted values for catgorical variables: uniform | 0.2,0.4,0.2,0.2
# To implement empirical
ancestor_prior_hp = uniform
rates_prior_hp = uniform
freqs_prior_hp = uniform
# accepted values for branch prior: 2 float values separated by comma (w/out space)
# mu and sigma for Lognormal (mean = exp(mu - ((sigma^2) / 2))
# alpha and beta (rate) for Gamma (mean = alpha/beta)
branch_prior_hp = 0.1,1.
kappa_prior_hp = 1.,1.

[settings]
# job_name will be updated automatically
job_name = nb3_l5k_datajc69_evojc69
# cpu | cuda | cuda:0
device = cpu
seed = 14
verbose = 1
compress_files = False

[plotting]
# To render Tex text in plots, Matplotlib requires
# Latex, dvipng, Ghostscript and type1ec.sty found in cm-super
# If Latex and other required packages are not installed,put to False
plt_usetex = False
size_font = 16
print_xtick_every = 1000

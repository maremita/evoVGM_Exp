## ##################################################################
## write_run_hyperpam_exps.py template config file 
## 
## Hyper-parameters and default values used in the 2022 BCB paper
## ##################################################################

## ##################################################################
## write_run_hyperpam_exps.py configuration sections
## ##################################################################

## [slurm] and [evaluation] sections will be removed in the final
## config files for evovgm.py

[slurm]
run_slurm=False
## if run_slurm=False: the script will launch the jobs locally
## elif run_slurm=True: it will launch the jobs on slurm

## SLURM parameters
## ################
exec_time=12:00:00
mem=75000M
cpus_per_task=12
## For testing
#exec_time = 00:05:00
#mem=8000M

[evaluation]
run_jobs=True
## if run_jobs = False: the script generates the evovgm config files 
## but it won't launch the jobs. If run_jobs=True: it runs the jobs

n_epochs=5000
nb_replicates=10
## For testing
#n_epochs=100
#nb_replicates=2

# Remark: values can span multiple lines, as long as they are
# indented deeper than the first line of the value.
# Configparser fetchs the value as string, after that it will be 
# casted to a list using json.loads

## Hyper-parameter grid
## ####################
##                [hyper-param name, grid values, default value]
hyper_types= {
            "kl": ["alpha_kl", [0.0001, 0.001, 0.01, 0.1], 0.0001],
            "hs": ["hidden_size", [4, 16, 32, 64], 32],
            "ns": ["nb_samples", [1, 10, 100, 1000], 100],
            "lr": ["learning_rate", 
                  [0.00005, 0.0005, 0.005, 0.05], 0.005]
            }

## Substitution model type for evoVGM model
## ############################################
#model_type = jc69
#model_type = k80 
model_type = gtr

## Simulation data parameters 
## ##########################
## [model name, substitution rates, relative frequencies]
#data_type = [
#    "jc69",
#    "0.160, 0.160, 0.160, 0.160, 0.160, 0.160",
#    "0.25, 0.25, 0.25, 0.25"
#    ]
#data_type = [
#    "k80" , 
#    "0.250, 0.125, 0.125, 0.125, 0.125, 0.250",
#    "0.25, 0.25, 0.25, 0.25"
#    ]
data_type = [
    "gtr" ,
    "0.160, 0.050, 0.160, 0.090, 0.300, 0.240",
    "0.10, 0.45, 0.30, 0.15"
    ]

## Number of sequences and their branch lengths
## ############################################
#branche = [3, "0.1,0.3,0.45"]
#branche = [4, "0.1,0.3,0.45,0.15"]
branche = [5, "0.1,0.3,0.45,0.15,0.2"]

## Alignment length
## ################
#len_aln = 100
#len_aln =  1000
len_aln = 5000

# ##################################################################
# EvoVGM template configuration sections
# ##################################################################
[io]
# output_path is used to save training data, scores and figures
# It will be updated to ../exp_outputs/<job_code>
# <job_code> is defined in write_run_rep_exps.py
output_path = nb_sequences

# If False the program runs the evaluation and save resutls in
# output_path else the program load results directly from the
# file (if it exists)
scores_from_file = True

[data]
# Use validation data along fitting and for generating step
validation = True

# If sim_data = True: evolve new sequences
# else: fetch sequences from fasta_fit_file [and fasta_val_file]
sim_data = True

# If sim_from_fasta and sim_data are True, the data will be
# extracted from simulated FASTA files if they already exist,
# else: new alignment will be simulated
sim_from_fasta = True

## alignment_size will be updated from [evaluation] section
alignment_size = 5000

## Evo parameters
## ##############
## The parameters will be updated from [evaluation] section
##
# Branch lengths is list of M floats separated by comma (w/out space)
# M is the number of sequences to be evolved
branch_lengths = 0.1,0.3,0.45
# Substitution rates
#        AG    AC    AT    GC    GT    CT
rates = 0.16, 0.16, 0.16, 0.16, 0.16, 0.16
# Relative frequencies
#        A     C      G    T
freqs = 0.25, 0.25, 0.25, 0.25

[vb_model]
## evomodel will be updated from [evaluation] section
# jc69 := EvoVGM_JC69: infer a and b latent variables
# k80  := EvoVGM_K80 : infer a, b, k latent variables
# gtr  := EvoVGM_GTR : infer a, b, r, f latent variables
evomodel = jc69

[hyperparams]
## Hyper-params will be updated from [evaluation] section
nb_replicates = 10
alpha_kl = 0.0001
nb_samples = 100
hidden_size = 32
nb_layers = 3
sample_temp = 0.1
n_epochs = 5000
# optimizer type : adam | sgd
optim=adam
learning_rate = 0.005
optim_weight_decay = 0.00001

[priors]
# Accepted values for catgorical variables: uniform | 0.2,0.4,0.2,0.2
# To implement empirical
ancestor_prior_hp = uniform
rates_prior_hp = uniform
freqs_prior_hp = uniform
# accepted values for branch prior: 2 float values separated by comma (w/out space)
# mu and sigma for Lognormal (mean = exp(mu - ((sigma^2) / 2))
# alpha and beta (rate) for Gamma (mean = alpha/beta)
branch_prior_hp = 0.1,1.
kappa_prior_hp = 1.,1.

[settings]
# job_name will be updated automatically
job_name = nb3_l5k_datajc69_evojc69
# cpu | cuda | cuda:0
device = cpu
seed = 14
verbose = 1

[plotting]
# To render Tex text in plots, Matplotlib requires
# Latex, dvipng, Ghostscript and type1ec.sty found in cm-super
# If Latex and other required packages are not installed,put to False
plt_usetex = False
size_font = 16
print_xtick_every = 1000
